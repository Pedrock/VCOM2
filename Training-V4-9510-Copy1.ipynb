{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing\n",
    "import tools.image as T\n",
    "\n",
    "from keras import __version__\n",
    "#from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "#from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from inception_v4 import InceptionV4, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from types import SimpleNamespace\n",
    "\n",
    "IM_WIDTH, IM_HEIGHT = 299, 299 # fixed size for InceptionV3, Inception ResNet V2 and Inception V4\n",
    "EPOCHS = 150\n",
    "BAT_SIZE = 16\n",
    "FC_SIZE = 1024\n",
    "#NB_LAYERS_TO_FREEZE = 172 # 165 ou ou 197\n",
    "\n",
    "#NB_LAYERS_TO_FREEZE = 249 # Inception V3\n",
    "#NB_LAYERS_TO_FREEZE = 618 # Inception ResNet V2\n",
    "NB_LAYERS_TO_FREEZE = 369 # Inception V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(weights_path='weights_checkpoint.h5', patience=30, monitor='val_loss'):\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=patience, monitor=monitor, min_delta=0.01)\n",
    "    model_checkpoint = ModelCheckpoint(weights_path,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=True,\n",
    "                                       monitor=monitor)\n",
    "    return [early_stopping, model_checkpoint]\n",
    "\n",
    "def get_nb_files(directory):\n",
    "    \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    cnt = 0\n",
    "    for r, dirs, files in os.walk(directory):\n",
    "        for dr in dirs:\n",
    "            cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def setup_to_transfer_learn(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "\n",
    "    Args:\n",
    "        base_model: keras model excluding top\n",
    "        nb_classes: # of classes\n",
    "\n",
    "    Returns:\n",
    "        new keras model with last layer\n",
    "    \"\"\"\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation = 'relu')(x) # new FC layer, random init\n",
    "    predictions = Dense(nb_classes, activation = 'softmax')(x) # new softmax layer\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_to_finetune(model):\n",
    "    \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the remaining top layers.\n",
    "\n",
    "    note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "\n",
    "    Args:\n",
    "        model: keras model\n",
    "    \"\"\"\n",
    "    for layer in model.layers[:NB_LAYERS_TO_FREEZE]:\n",
    "         layer.trainable = False\n",
    "    for layer in model.layers[NB_LAYERS_TO_FREEZE:]:\n",
    "         layer.trainable = True\n",
    "    model.compile(optimizer = SGD(lr = 0.0001, momentum = 0.9), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    \"\"\"Use transfer learning and fine-tuning to train a network on a new dataset\"\"\"\n",
    "    nb_train_samples = get_nb_files(args.train_dir)\n",
    "    nb_classes = len(glob.glob(args.train_dir + \"/*\"))\n",
    "    nb_val_samples = get_nb_files(args.val_dir)\n",
    "    epochs = int(args.epochs)\n",
    "    batch_size = int(args.batch_size)\n",
    "    \n",
    "    '''try:\n",
    "        pool.terminate()\n",
    "    except:\n",
    "        pass\n",
    "    n_process = 6\n",
    "    pool = multiprocessing.Pool(processes = n_process)'''\n",
    "\n",
    "    # data prep\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        rotation_range = 45,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        #pool = pool\n",
    "    )\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        #rotation_range = 90,\n",
    "        #width_shift_range = 0.2,\n",
    "        #height_shift_range = 0.2,\n",
    "        #shear_range = 0.2,\n",
    "        #zoom_range = 0.2,\n",
    "        #horizontal_flip = True,\n",
    "        #pool = pool\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        args.train_dir,\n",
    "        target_size = (IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size = batch_size,\n",
    "    )\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        args.val_dir,\n",
    "        target_size = (IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size = batch_size,\n",
    "    )\n",
    "\n",
    "    # setup model\n",
    "    #base_model = InceptionV3(weights = 'imagenet', include_top = False)\n",
    "    #base_model = InceptionResNetV2(weights = 'imagenet', include_top = False)\n",
    "    base_model = InceptionV4(weights = 'imagenet', include_top = False)\n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "    # transfer learning\n",
    "    setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "    history_tl = model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs = epochs,\n",
    "        steps_per_epoch = nb_train_samples / batch_size,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = nb_val_samples / batch_size,\n",
    "        class_weight = 'auto',\n",
    "        callbacks = get_callbacks())\n",
    "\n",
    "    # fine-tuning\n",
    "    setup_to_finetune(model)\n",
    "\n",
    "    history_ft = model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs = epochs,\n",
    "        steps_per_epoch = nb_train_samples / batch_size,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = nb_val_samples / batch_size,\n",
    "        class_weight = 'auto',\n",
    "        callbacks = get_callbacks())\n",
    "\n",
    "    model.save(args.output_model_file)\n",
    "\n",
    "    if args.plot:\n",
    "        plot_training(history_ft)\n",
    "\n",
    "\n",
    "def plot_training(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'r.')\n",
    "    plt.plot(epochs, val_acc, 'r')\n",
    "    plt.title('Training and validation accuracy')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'r.')\n",
    "    plt.plot(epochs, val_loss, 'r-')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 30 classes.\n",
      "Found 2000 images belonging to 30 classes.\n",
      "Epoch 1/150\n",
      "500/500 [==============================] - 229s 459ms/step - loss: 2.7652 - acc: 0.4109 - val_loss: 2.2566 - val_acc: 0.5650\n",
      "Epoch 2/150\n",
      "500/500 [==============================] - 214s 427ms/step - loss: 2.0646 - acc: 0.6005 - val_loss: 1.9667 - val_acc: 0.6505\n",
      "Epoch 3/150\n",
      "500/500 [==============================] - 213s 425ms/step - loss: 1.9019 - acc: 0.6514 - val_loss: 1.9307 - val_acc: 0.7025\n",
      "Epoch 4/150\n",
      "500/500 [==============================] - 212s 425ms/step - loss: 1.8348 - acc: 0.6725 - val_loss: 1.6474 - val_acc: 0.7435\n",
      "Epoch 5/150\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 1.7917 - acc: 0.6862 - val_loss: 1.6535 - val_acc: 0.7525\n",
      "Epoch 6/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.7627 - acc: 0.6951 - val_loss: 1.6753 - val_acc: 0.7440\n",
      "Epoch 7/150\n",
      "500/500 [==============================] - 215s 429ms/step - loss: 1.7391 - acc: 0.7026 - val_loss: 1.5991 - val_acc: 0.7715\n",
      "Epoch 8/150\n",
      "500/500 [==============================] - 213s 427ms/step - loss: 1.7082 - acc: 0.7126 - val_loss: 1.5305 - val_acc: 0.7780\n",
      "Epoch 9/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.7013 - acc: 0.7200 - val_loss: 1.7235 - val_acc: 0.7480\n",
      "Epoch 10/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.7071 - acc: 0.7133 - val_loss: 1.6589 - val_acc: 0.7465\n",
      "Epoch 11/150\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 1.6650 - acc: 0.7250 - val_loss: 1.6543 - val_acc: 0.7810\n",
      "Epoch 12/150\n",
      "500/500 [==============================] - 211s 423ms/step - loss: 1.6850 - acc: 0.7192 - val_loss: 1.5861 - val_acc: 0.7785\n",
      "Epoch 13/150\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 1.6665 - acc: 0.7292 - val_loss: 1.8203 - val_acc: 0.7355\n",
      "Epoch 14/150\n",
      "500/500 [==============================] - 212s 425ms/step - loss: 1.6710 - acc: 0.7351 - val_loss: 1.7747 - val_acc: 0.7415\n",
      "Epoch 15/150\n",
      "500/500 [==============================] - 213s 426ms/step - loss: 1.6320 - acc: 0.7400 - val_loss: 1.7414 - val_acc: 0.7520\n",
      "Epoch 16/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6753 - acc: 0.7274 - val_loss: 2.0314 - val_acc: 0.7145\n",
      "Epoch 17/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6422 - acc: 0.7449 - val_loss: 1.6926 - val_acc: 0.7635\n",
      "Epoch 18/150\n",
      "500/500 [==============================] - 212s 423ms/step - loss: 1.6241 - acc: 0.7435 - val_loss: 1.9403 - val_acc: 0.7380\n",
      "Epoch 19/150\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 1.6512 - acc: 0.7389 - val_loss: 2.1419 - val_acc: 0.7220\n",
      "Epoch 20/150\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 1.6431 - acc: 0.7444 - val_loss: 1.8421 - val_acc: 0.7630\n",
      "Epoch 21/150\n",
      "500/500 [==============================] - 211s 421ms/step - loss: 1.6398 - acc: 0.7464 - val_loss: 1.7848 - val_acc: 0.7520\n",
      "Epoch 22/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6335 - acc: 0.7411 - val_loss: 1.9799 - val_acc: 0.7285\n",
      "Epoch 23/150\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 1.6290 - acc: 0.7513 - val_loss: 1.9598 - val_acc: 0.7450\n",
      "Epoch 24/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6143 - acc: 0.7472 - val_loss: 1.9419 - val_acc: 0.7400\n",
      "Epoch 25/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6429 - acc: 0.7423 - val_loss: 1.9684 - val_acc: 0.7365\n",
      "Epoch 26/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6229 - acc: 0.7466 - val_loss: 2.0675 - val_acc: 0.7135\n",
      "Epoch 27/150\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 1.6249 - acc: 0.7520 - val_loss: 1.9976 - val_acc: 0.7295\n",
      "Epoch 28/150\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 1.6089 - acc: 0.7550 - val_loss: 2.0166 - val_acc: 0.7330\n",
      "Epoch 29/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6133 - acc: 0.7518 - val_loss: 2.0616 - val_acc: 0.7300\n",
      "Epoch 30/150\n",
      "500/500 [==============================] - 213s 426ms/step - loss: 1.6247 - acc: 0.7470 - val_loss: 2.0770 - val_acc: 0.7260\n",
      "Epoch 31/150\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 1.6099 - acc: 0.7515 - val_loss: 2.0188 - val_acc: 0.7400\n",
      "Epoch 32/150\n",
      "500/500 [==============================] - 212s 425ms/step - loss: 1.6096 - acc: 0.7546 - val_loss: 1.9397 - val_acc: 0.7525\n",
      "Epoch 33/150\n",
      "500/500 [==============================] - 213s 425ms/step - loss: 1.6312 - acc: 0.7524 - val_loss: 1.9051 - val_acc: 0.7620\n",
      "Epoch 34/150\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 1.6033 - acc: 0.7594 - val_loss: 2.1157 - val_acc: 0.7340\n",
      "Epoch 35/150\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.6115 - acc: 0.7584"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    train_dir = \"C:\\\\AID\\\\AID-split\\\\train\",\n",
    "    val_dir = \"C:\\\\AID\\\\AID-split\\\\test\",\n",
    "    #output_model_file = \"inceptionv3-ft-2.h5\",\n",
    "    #output_model_file = \"inception_resnet_v2-ft.h5\",\n",
    "    output_model_file = \"inceptionv4-ft.h5\",\n",
    "    epochs = EPOCHS,\n",
    "    batch_size = BAT_SIZE,\n",
    "    plot = True)\n",
    "\n",
    "if (not os.path.exists(args.train_dir)) or (not os.path.exists(args.val_dir)):\n",
    "    print(\"directories do not exist\")\n",
    "    sys.exit(1)\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_v4 import InceptionV4, preprocess_input\n",
    "base_model = InceptionV4(weights = 'imagenet', include_top = False)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
