{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning an Inception V4 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "from keras import __version__\n",
    "from inception_v4 import InceptionV4, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from types import SimpleNamespace\n",
    "\n",
    "IM_WIDTH, IM_HEIGHT = 299, 299 # fixed size for Inception V4\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "FC_SIZE = 1024\n",
    "TRAIN_DIR = \"C:\\\\AID\\\\AID-split\\\\train\"\n",
    "TEST_DIR = \"C:\\\\AID\\\\AID-split\\\\test\"\n",
    "WEIGHTS_PATH = str(os.path.join('output_weights', 'weights_checkpoint.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(weights_path=WEIGHTS_PATH, patience=40, monitor='val_acc', min_delta=0):\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=patience, monitor=monitor, min_delta=min_delta)\n",
    "    model_checkpoint = ModelCheckpoint(weights_path,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=True,\n",
    "                                       monitor=monitor)\n",
    "    return [early_stopping, model_checkpoint]\n",
    "\n",
    "\n",
    "def get_nb_files(directory):\n",
    "    \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    cnt = 0\n",
    "    for r, dirs, files in os.walk(directory):\n",
    "        for dr in dirs:\n",
    "            cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def setup_to_transfer_learn(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\"\"\"\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation = 'relu')(x) # new FC layer, random init\n",
    "    predictions = Dense(nb_classes, activation = 'softmax')(x) # new softmax layer\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_to_finetune(model):\n",
    "    \"\"\"Unfreeze all layers\"\"\"\n",
    "    for layer in model.layers:\n",
    "         layer.trainable = True\n",
    "    model.compile(optimizer = SGD(lr = 0.0001, momentum = 0.9), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "def plot_training(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    if not os.path.exists('output_graphs'):\n",
    "        os.makedirs('output_graphs')\n",
    "\n",
    "    plt.plot(epochs, acc, 'r.')\n",
    "    plt.plot(epochs, val_acc, 'r')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    \n",
    "    plt.savefig(os.path.join('output_graphs', 'training_plot1_' + str(time.time()).split('.')[0] + '.svg'))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'r.')\n",
    "    plt.plot(epochs, val_loss, 'r-')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join('output_graphs', 'training_plot2_' + str(time.time()).split('.')[0] + '.svg'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use transfer learning and fine-tuning to train a network on a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test directories must exist\n",
    "assert(os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR))\n",
    "\n",
    "nb_train_samples = get_nb_files(TRAIN_DIR)\n",
    "nb_classes = len(glob.glob(TRAIN_DIR + \"/*\"))\n",
    "nb_val_samples = get_nb_files(TEST_DIR)\n",
    "\n",
    "if not os.path.exists('output_weights'):\n",
    "    os.makedirs('output_weights')\n",
    "\n",
    "# data prep\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input,\n",
    "    rotation_range = 45,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size = (IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size = BATCH_SIZE,\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size = (IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size = BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# setup model\n",
    "base_model = InceptionV4(weights = 'imagenet', include_top = False)\n",
    "model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "# transfer learning\n",
    "setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "history_tl = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs = EPOCHS,\n",
    "    steps_per_epoch = nb_train_samples / BATCH_SIZE,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_val_samples / BATCH_SIZE,\n",
    "    class_weight = 'auto',\n",
    "    callbacks = get_callbacks(monitor='val_loss', min_delta=0.075)\n",
    ")\n",
    "\n",
    "# Load the best weights from the previous stage\n",
    "model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "# fine-tuning\n",
    "setup_to_finetune(model)\n",
    "\n",
    "history_ft = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs = EPOCHS,\n",
    "    steps_per_epoch = nb_train_samples / BATCH_SIZE,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_val_samples / BATCH_SIZE,\n",
    "    class_weight = 'auto',\n",
    "    callbacks = get_callbacks())\n",
    "\n",
    "model.save_weights(os.path.join('output_weights', 'weights_' + str(time.time()).split('.')[0] + '.h5'))\n",
    "\n",
    "if args.plot:\n",
    "    plot_training(history_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
